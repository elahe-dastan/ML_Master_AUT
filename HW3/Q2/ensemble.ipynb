{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"p2_data/data_train.csv\", header=None)\n",
    "target = train.shape[1]-1\n",
    "y = train[target].to_numpy()\n",
    "X = train.drop(columns=[target]).to_numpy()\n",
    "\n",
    "test = pd.read_csv(\"p2_data/data_test.csv\", header=None)\n",
    "y_test = test[target]\n",
    "X_test = test.drop(columns=[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A)<br/>\n",
    "Implement Random Forest:\n",
    "1. It gets the number of trees (n) and their height\n",
    "2. Samples the dataset with replacement(bootstrap) n times\n",
    "3. For each tree randomly selects a few features\n",
    "4. Train all trees\n",
    "5. sum the predictions of all trees and use argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n, depth):\n",
    "        self.n = n\n",
    "        self.depth = depth\n",
    "        self.trees = []\n",
    "        self.features = []\n",
    "        self.samplesX = []\n",
    "        self.samplesy = []\n",
    "        self.n_classes = 2\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        for i in range(self.n):\n",
    "            clf = tree.DecisionTreeClassifier(max_depth=self.depth)\n",
    "            self.trees.append(clf)\n",
    "            \n",
    "    def fit(self, X, y, n):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        for i in range(self.n):\n",
    "            self.bootstrap(X, y)\n",
    "            self.random_features(n, X.shape[1]-1)\n",
    "            self.trees[i] = self.trees[i].fit(self.samplesX[i][:, self.features[i]], self.samplesy[i])\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(shape=(len(X), self.n_classes))\n",
    "        for i in range(self.n):\n",
    "            predictions = np.add(predictions,self.trees[i].predict_proba(X.iloc[:, self.features[i]]))\n",
    "            \n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "    def bootstrap(self, X, y):\n",
    "        r = random.randint(0, 1000)\n",
    "        bootX = resample(X, replace=True, n_samples=len(X), random_state=r)\n",
    "        booty = resample(y, replace=True, n_samples=len(X), random_state=r)\n",
    "        self.samplesX.append(bootX)\n",
    "        self.samplesy.append(booty)\n",
    "        \n",
    "    def random_features(self, n, n_features):\n",
    "        features = []\n",
    "        for i in range(n):\n",
    "            features.append(random.randint(0, n_features))\n",
    "        self.features.append(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForest(15, 3)\n",
    "clf.fit(X, y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       8\n",
      "1       8\n",
      "2       8\n",
      "3       9\n",
      "4       9\n",
      "       ..\n",
      "3493    4\n",
      "3494    2\n",
      "3495    0\n",
      "3496    0\n",
      "3497    4\n",
      "Name: 16, Length: 3498, dtype: int64\n",
      "[8 8 0 ... 0 0 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "predictions = clf.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7375643224699828"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[334,   2,   0,   0,   0,   0,   0,   1,  26,   0],\n",
       "       [  0, 189, 149,  17,   3,   0,   3,   0,   0,   3],\n",
       "       [  0,  10, 346,   3,   0,   0,   0,   5,   0,   0],\n",
       "       [  0,   8,   0, 318,   0,   0,   1,   8,   0,   1],\n",
       "       [  0,   1,   0,   0, 357,   0,   3,   0,   0,   3],\n",
       "       [  4,   1,   0, 126,  11, 156,   8,   0,   8,  21],\n",
       "       [  3,   0,   0,   3,   3,   0, 324,   3,   0,   0],\n",
       "       [  0,  53,   8,   1,   4,   0,   4, 283,  11,   0],\n",
       "       [ 85,   0,   0,   0,   0,  36,   1,  14, 200,   0],\n",
       "       [  0,  46,   0, 176,  33,   0,   4,   3,   1,  73]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B)<br/>\n",
    "Implement AdaBoost:<br/>\n",
    "In this implementation I found argmax more meaningfull than sign method because we are facing a multiclass problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.trees = [] \n",
    "        self.alphas = []\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        for i in range(self.n):\n",
    "            stump = tree.DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)\n",
    "            self.trees.append(stump)\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        n_samples, _ = X.shape\n",
    "        \n",
    "        # init weight\n",
    "        w = np.full(n_samples, (1/n_samples))\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            self.trees[i] = self.trees[i].fit(X, y, sample_weight=w)\n",
    "            predictions = self.trees[i].predict(X)\n",
    "            \n",
    "            missclassified = w[y != predictions]\n",
    "            error = sum(missclassified)\n",
    "            \n",
    "            EPS = 1e-10\n",
    "            alpha = 0.5 * np.log((1-error) / (error+EPS))\n",
    "            self.alphas.append(alpha)\n",
    "           \n",
    "            w *= np.exp(-alpha * y * predictions)\n",
    "            w /= np.sum(w)\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(shape=(X.shape[0], 10))\n",
    "        for i in range(self.n):\n",
    "            predictions = np.add(predictions, self.alphas[i] * self.trees[i].predict_proba(X))\n",
    "\n",
    "        return np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10377358490566038"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoost(10)\n",
    "clf.fit(X, y)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaBoost algorithm I implemented predicts every sample zero. After some searching I found out that after traing a few trees, there is one tree that predicts every sample zero and from then the error doesn't change much so the weights don't change either and all the trees after this predict only zero.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19925671812464266\n",
      "0.10377358490566038\n"
     ]
    }
   ],
   "source": [
    "clf_5 = AdaBoost(5)\n",
    "clf_5.fit(X, y)\n",
    "\n",
    "predictions = clf_5.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "\n",
    "clf_20 = AdaBoost(20)\n",
    "clf_20.fit(X, y)\n",
    "\n",
    "predictions = clf_20.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "\n",
    "clf_50 = AdaBoost(50)\n",
    "clf_50.fit(X, y)\n",
    "\n",
    "predictions = clf_50.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an implementation of gradient boosting so explaining gradient boosting also explains xgboost.<br/>\n",
    "XGBoost classifier seems like AdaBoost classifier.The major difference between AdaBoost and Gradient Boosting Algorithm is how the two algorithms identify the shortcomings of weak learners (eg. decision trees). While the AdaBoost model identifies the shortcomings by using high weight data points, gradient boosting performs the same by using gradients in the loss function. The loss function is a measure indicating how good modelâ€™s coefficients are at fitting the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}