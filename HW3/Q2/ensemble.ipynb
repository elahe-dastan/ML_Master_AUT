{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"p2_data/data_train.csv\", header=None)\n",
    "target = train.shape[1]-1\n",
    "y = train[target].to_numpy()\n",
    "X = train.drop(columns=[target]).to_numpy()\n",
    "\n",
    "test = pd.read_csv(\"p2_data/data_test.csv\", header=None)\n",
    "y_test = test[target]\n",
    "X_test = test.drop(columns=[target])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A)<br/>\n",
    "Implement Random Forest:\n",
    "1. It gets the number of trees (n) and their height\n",
    "2. Samples the dataset with replacement(bootstrap) n times\n",
    "3. For each tree randomly selects a few features\n",
    "4. Train all trees\n",
    "5. sum the predictions of all trees and use argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n, depth):\n",
    "        self.n = n\n",
    "        self.depth = depth\n",
    "        self.trees = []\n",
    "        self.features = []\n",
    "        self.samplesX = []\n",
    "        self.samplesy = []\n",
    "        self.n_classes = 2\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        for i in range(self.n):\n",
    "            clf = tree.DecisionTreeClassifier(max_depth=self.depth)\n",
    "            self.trees.append(clf)\n",
    "            \n",
    "    def fit(self, X, y, n):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        for i in range(self.n):\n",
    "            self.bootstrap(X, y)\n",
    "            self.random_features(n, X.shape[1]-1)\n",
    "            self.trees[i] = self.trees[i].fit(self.samplesX[i][:, self.features[i]], self.samplesy[i])\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(shape=(len(X), self.n_classes))\n",
    "        for i in range(self.n):\n",
    "            predictions = np.add(predictions,self.trees[i].predict_proba(X.iloc[:, self.features[i]]))\n",
    "            \n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "    def bootstrap(self, X, y):\n",
    "        r = random.randint(0, 1000)\n",
    "        bootX = resample(X, replace=True, n_samples=len(X), random_state=r)\n",
    "        booty = resample(y, replace=True, n_samples=len(X), random_state=r)\n",
    "        self.samplesX.append(bootX)\n",
    "        self.samplesy.append(booty)\n",
    "        \n",
    "    def random_features(self, n, n_features):\n",
    "        features = []\n",
    "        for i in range(n):\n",
    "            features.append(random.randint(0, n_features))\n",
    "        self.features.append(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForest(15, 3)\n",
    "clf.fit(X, y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       8\n",
      "1       8\n",
      "2       8\n",
      "3       9\n",
      "4       9\n",
      "       ..\n",
      "3493    4\n",
      "3494    2\n",
      "3495    0\n",
      "3496    0\n",
      "3497    4\n",
      "Name: 16, Length: 3498, dtype: int64\n",
      "[8 8 0 ... 0 0 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "predictions = clf.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7655803316180675"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[311,   6,   0,   0,   1,   0,  17,   3,  25,   0],\n",
       "       [  0, 144, 156,  59,   2,   0,   2,   0,   0,   1],\n",
       "       [  0,   4, 357,   0,   0,   0,   1,   2,   0,   0],\n",
       "       [  0,   6,   1, 326,   0,   0,   0,   2,   0,   1],\n",
       "       [  0,   3,   0,   0, 355,   0,   5,   0,   0,   1],\n",
       "       [  1,   0,   1,  98,   3, 166,  20,   0,   1,  45],\n",
       "       [  2,   0,   2,   0,   2,   0, 325,   5,   0,   0],\n",
       "       [  4,  32,  11,   1,  20,   0,   0, 293,   3,   0],\n",
       "       [ 28,   0,   0,   0,   0,   6,   1,  23, 277,   1],\n",
       "       [  0,  39,   0, 147,  19,   1,   6,   0,   0, 124]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B)<br/>\n",
    "Implement AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.trees = [] \n",
    "        self.alphas = []\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        for i in range(self.n):\n",
    "            stump = tree.DecisionTreeClassifier(max_depth=1)\n",
    "            self.trees.append(stump)\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        n_samples, _ = X.shape\n",
    "        \n",
    "        # init weight\n",
    "        w = np.full(n_samples, (1/n_samples))\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            self.trees[i] = self.trees[i].fit(X, y, sample_weight=w)\n",
    "            predictions = self.trees[i].predict(X)\n",
    "\n",
    "#             print(predictions)\n",
    "            \n",
    "            missclassified = w[y != predictions]\n",
    "            error = sum(missclassified)\n",
    "            \n",
    "            print(np.sum(w[y == predictions]))\n",
    "            print(np.sum(w[y != predictions]))\n",
    "            print(\"++++++++++++\")\n",
    "            \n",
    "            EPS = 1e-10\n",
    "            alpha = 0.5 * np.log((1-error) / (error+EPS))\n",
    "            self.alphas.append(alpha)\n",
    "           \n",
    "            w *= np.exp(-alpha * y * predictions)\n",
    "            w /= np.sum(w)\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([])\n",
    "        for i in range(self.n):\n",
    "            np.append(predictions, self.alphas[i] * self.trees[i].predict_proba(X))\n",
    "        print(predictions)\n",
    "        \n",
    "        y_pred = np.sum(predictions, axis=0)\n",
    "        return np.sign(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2068321323725647\n",
      "0.7931678676274352\n",
      "++++++++++++\n",
      "0.902268603311021\n",
      "0.09773139668897887\n",
      "++++++++++++\n",
      "0.9999881625985556\n",
      "1.1837401444314504e-05\n",
      "++++++++++++\n",
      "0.9999914273824037\n",
      "8.572617596505707e-06\n",
      "++++++++++++\n",
      "0.9999919703297782\n",
      "8.029670221883996e-06\n",
      "++++++++++++\n",
      "0.9999919687556711\n",
      "8.03124432881917e-06\n",
      "++++++++++++\n",
      "0.9999919687556711\n",
      "8.03124432881917e-06\n",
      "++++++++++++\n",
      "0.9999919687556711\n",
      "8.03124432881917e-06\n",
      "++++++++++++\n",
      "0.9999919687556711\n",
      "8.03124432881917e-06\n",
      "++++++++++++\n",
      "0.9999919687556711\n",
      "8.03124432881917e-06\n",
      "++++++++++++\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoost(10)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# print(y)\n",
    "predictions = clf.predict(X_test)\n",
    "# print(predictions)\n",
    "\n",
    "# accuracy_score(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2068321323725647\n",
      "0.7931678676274352\n",
      "++++++++++++\n",
      "0.902268603311021\n",
      "0.09773139668897887\n",
      "++++++++++++\n",
      "0.9999881625985556\n",
      "1.1837401444314504e-05\n",
      "++++++++++++\n",
      "0.9999914273824037\n",
      "8.572617596505707e-06\n",
      "++++++++++++\n",
      "0.9999919703297782\n",
      "8.029670221883996e-06\n",
      "++++++++++++\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3498, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1fd215fd465a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclf_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    257\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3498, 0]"
     ]
    }
   ],
   "source": [
    "clf_5 = AdaBoost(5)\n",
    "clf_5.fit(X, y)\n",
    "\n",
    "predictions = clf_5.predict(X_test)\n",
    "accuracy_score(y_test, predictions)\n",
    "\n",
    "clf_20 = AdaBoost(20)\n",
    "clf_20.fit(X, y)\n",
    "\n",
    "predictions = clf_20.predict(X_test)\n",
    "accuracy_score(y_test, predictions)\n",
    "\n",
    "clf_50 = AdaBoost(50)\n",
    "clf_50.fit(X, y)\n",
    "\n",
    "predictions = clf_50.predict(X_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-deabec2b97a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
