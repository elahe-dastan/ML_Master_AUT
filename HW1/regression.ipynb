{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this HW we had to implement regression and gradient descent ourselves in python. I've done this before in Golang which tended to be faster (check [newborn repo](https://github.com/elahe-dastan/newborn) if intersted) because Golang is faster than python I didn't do any test but if you are about to do something similar pay attention that the vectorization used in python like what numpy does most of the time boosts the speed which we may not have in other languages.<br/>\n",
    "knowing the simple math behind these algorithms is enough for implementation:<br/>\n",
    "[check here for the math behind](https://elahe-dastan.github.io/newborn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "class regression:\n",
    "    GRADIENT_DESCENT = 'gradient_descent'\n",
    "    NORMAL_EQUATION = 'normal_equation'\n",
    "    def __init__(self, degree: int, method=GRADIENT_DESCENT):\n",
    "        self.degree = degree\n",
    "        # Example:\n",
    "        # if the training dataset has feature x and the degree is 2 it uses both x and x^2 for training.\n",
    "        self.method = method\n",
    "        self.coefficients = None\n",
    "        self.steps = np.array([])\n",
    "        \n",
    "        self.train_error_mse = np.array([])\n",
    "        self.test_error_mse = np.array([])\n",
    "        \n",
    "        self.train_error_mae = np.array([])\n",
    "        self.test_error_mae = np.array([])\n",
    "        \n",
    "        self.train_error_rmse = np.array([])\n",
    "        self.test_error_rmse = np.array([])\n",
    "    \n",
    "    def initialization(self, x):\n",
    "        features_num = x.shape[1]\n",
    "        \n",
    "        for i in range(self.degree-1):\n",
    "            x = np.c_[x, np.multiply(x[:, 0:features_num], x[:, i*features_num:(i+1)*features_num])]\n",
    "        \n",
    "        # putting a vector of one in the first column for bias\n",
    "        x = np.c_[np.ones(len(x)), x]\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def train(self, x: np.array, y: np.array, learning_rate=0.1, iteration=10000, lam=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = iteration\n",
    "        features_num = x.shape[1]\n",
    "        # plus 1 is for considering bias\n",
    "        self.coefficients = np.random.random((self.degree * features_num) + 1)\n",
    "        \n",
    "        if self.method == self.GRADIENT_DESCENT:\n",
    "            self.__train_gradient_descent(x, y, learning_rate, iteration)\n",
    "        elif self.method == self.NORMAL_EQUATION:\n",
    "            self.__normal_equation(x, y, lam)\n",
    "        else:\n",
    "            print('I only know the methods {:s} and {:s}'.format(self.GRADIENT_DESCENT, self.NORMAL_EQUATION))\n",
    "        \n",
    "    def __train_gradient_descent(self, x: np.array, y: np.array, learning_rate: float, iteration: int):\n",
    "        x = self.initialization(x)\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            self.gradient_descent(x, y, learning_rate)\n",
    "            return\n",
    "    \n",
    "    def gradient_descent(self, x, y, learning_rate: float):\n",
    "        m = len(x)\n",
    "        prediction = x.dot(self.coefficients)\n",
    "        self.coefficients -= learning_rate * 1/m * (x.T.dot(prediction - y))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = self.initialization(x)\n",
    "        \n",
    "        return x.dot(self.coefficients)\n",
    "    \n",
    "    def __normal_equation(self, x, y, lam: float):\n",
    "        features_num = x.shape[1]\n",
    "        x = self.initialization(x)\n",
    "        self.x = x\n",
    "        mat = np.eye((self.degree * features_num) + 1)\n",
    "        mat[0][0] = 0\n",
    "\n",
    "        self.coefficients = np.linalg.inv(x.T.dot(x) + lam * mat).dot(x.T.dot(y))\n",
    "    \n",
    "    # Overfitting means the error on training data is reducing but it start to increase on test data\n",
    "    # so the model can't generalize I could check it outside the class by training several times with\n",
    "    # different number of iterations but this has a lower cost computationally.\n",
    "    # Example:\n",
    "    # if iteration = 10000 and step = 100 then the functions measures error each 100 iterations so\n",
    "    # it measures it 10000/100 = 100 times\n",
    "    def track_overfitting(self, X_train, X_test, y_train, y_test, learning_rate, iteration: int, step: int):\n",
    "        features_num = X_train.shape[1]\n",
    "        # plus 1 is for considering bias\n",
    "        self.coefficients = np.random.random((self.degree * features_num) + 1)\n",
    "        \n",
    "        X_train = self.initialization(X_train)\n",
    "        X_test = self.initialization(X_test)\n",
    "        \n",
    "        while iteration > step:\n",
    "            for i in range(step):\n",
    "                self.gradient_descent(X_train, y_train, learning_rate)\n",
    "                \n",
    "            self.error_measurement(X_train, X_test, y_train, y_test, step) \n",
    "            \n",
    "            iteration -= step\n",
    "\n",
    "        for i in range(iteration):\n",
    "            self.gradient_descent(X_train, y_train, learning_rate)    \n",
    "        \n",
    "        self.error_measurement(X_train, X_test, y_train, y_test, step)\n",
    "        \n",
    "        self.plot_error(iteration)\n",
    "           \n",
    "    def error_measurement(self, X_train, X_test, y_train, y_test, step):\n",
    "        if len(self.steps) == 0:\n",
    "            self.steps = np.append(self.steps, 0)\n",
    "        else:\n",
    "            self.steps = np.append(self.steps, self.steps[len(self.steps) - 1] + step)\n",
    "            \n",
    "        y_train_predicted = X_train.dot(self.coefficients)\n",
    "        self.train_error_mse = np.append(self.train_error_mse, mean_squared_error(y_train, y_train_predicted))\n",
    "        self.train_error_mae = np.append(self.train_error_mae, mean_absolute_error(y_train, y_train_predicted))\n",
    "        self.train_error_rmse = np.append(self.train_error_rmse, sqrt(mean_squared_error(y_train, y_train_predicted)))  \n",
    "            \n",
    "        y_test_predicted = X_test.dot(self.coefficients)\n",
    "        self.test_error_mse = np.append(self.test_error_mse, mean_squared_error(y_test, y_test_predicted))\n",
    "        self.test_error_mae = np.append(self.test_error_mae, mean_absolute_error(y_test, y_test_predicted))\n",
    "        self.test_error_rmse = np.append(self.test_error_rmse, sqrt(mean_squared_error(y_test, y_test_predicted)))\n",
    "    \n",
    "    def plot_error(self, iteration):\n",
    "        plt.scatter(self.steps, self.train_error_mse, c='blue')\n",
    "        plt.scatter(self.steps, self.test_error_mse, c='red')\n",
    "        plt.title('mean squared error')\n",
    "        plt.xlabel('steps')\n",
    "        plt.ylabel('error')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.scatter(self.steps, self.train_error_mae, c='blue')\n",
    "        plt.scatter(self.steps, self.test_error_mae, c='red')\n",
    "        plt.title('mean absolute error')\n",
    "        plt.xlabel('steps')\n",
    "        plt.ylabel('error')\n",
    "        plt.show()\n",
    "                                    \n",
    "        plt.scatter(self.steps, self.train_error_rmse, c='blue')\n",
    "        plt.scatter(self.steps, self.test_error_rmse, c='red')\n",
    "        plt.title('root mean squared error')\n",
    "        plt.xlabel('steps')\n",
    "        plt.ylabel('error')\n",
    "        plt.show() \n",
    "        \n",
    "        print('for the degree {:d} and {:d} iterations the errors for training set are MSE = {}, '\n",
    "              'MAE = {} and RMSE = {} and the errors for test set are MSE = {}, MAE = {} and RMSE = {}'\n",
    "              .format(self.degree, iteration, self.train_error_mse[len(self.train_error_mse)-1],\n",
    "                      self.train_error_mae[len(self.train_error_mae)-1], self.train_error_rmse[len(self.train_error_rmse)-1],\n",
    "                     self.test_error_mse[len(self.test_error_mse)-1], self.test_error_mae[len(self.test_error_mae)-1],\n",
    "                      self.test_error_rmse[len(self.test_error_rmse)-1]))\n",
    "\n",
    "    def plot_fitted(self, X_train, y_train, X_test):\n",
    "        plt.scatter(X_train, y_train, c='blue')\n",
    "        plt.scatter(X_test, reg.predict(np.array(X_test).reshape(len(X_test), 1)), c='red')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}